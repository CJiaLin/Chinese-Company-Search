{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "total_data = []\n",
    "with open('data/shbank_company_split.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        l = json.loads(line)\n",
    "        total_data.append(l)\n",
    "# sample_data = np.random.choice(total_data, int(len(total_data) * 0.01), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_place = []\n",
    "non_brand = []\n",
    "non_trade = []\n",
    "non_suffix = []\n",
    "for idx, item in zip(range(len(total_data)), total_data):\n",
    "    if 'place' not in item['label'] or item['label']['place'] == []:\n",
    "        non_place.append(idx)\n",
    "    if 'brand' not in item['label'] or item['label']['brand'] == []:\n",
    "        non_brand.append(idx)\n",
    "    if 'trade' not in item['label'] or item['label']['trade'] == []:\n",
    "        non_trade.append(idx)\n",
    "    if 'suffix' not in item or item['label']['suffix'] == []:\n",
    "        non_suffix.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "norma_idx = set(range(len(total_data))) - (set(non_place) | set(non_brand) | set(non_trade))  #  字段全的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7935\n",
      "1861\n",
      "4432\n",
      "89286\n"
     ]
    }
   ],
   "source": [
    "print(len(non_place))\n",
    "print(len(non_brand))\n",
    "print(len(non_trade))\n",
    "print(len(norma_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_list = list(set(non_place) & set(non_brand) & set(non_trade))\n",
    "# idx_list.sort()\n",
    "# idx_list.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_list = list(set(non_place) & set(non_brand))\n",
    "# idx_list.sort()\n",
    "# idx_list.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in idx_list:\n",
    "#     total_data.pop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in non_suffix:\n",
    "#     total_data.pop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "norma_idx_choice = np.random.choice(list(norma_idx), int(len(total_data) * 0.05), replace=False)\n",
    "non_place_choice = np.random.choice(list(non_place), int(len(non_place) * 0.1), replace=False)\n",
    "non_brand_choice = np.random.choice(list(non_brand), int(len(non_brand) * 0.1), replace=False)\n",
    "non_trade_choice = np.random.choice(list(non_trade), int(len(non_trade) * 0.1), replace=False)\n",
    "non_place_trade = list(set(non_place) & set(non_trade))\n",
    "non_place_trade_choice = np.random.choice(non_place_trade, int(len(non_place_trade) * 0.1), replace=False)\n",
    "non_brand_trade = list(set(non_brand) & set(non_trade))\n",
    "non_brand_trade_choice = np.random.choice(non_brand_trade, int(len(non_brand_trade) * 0.1), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_id = list(set(norma_idx_choice) | set(non_place_choice) | set(non_brand_choice) | set(non_trade_choice) \\\n",
    "                | set(non_place_trade_choice) | set(non_brand_trade_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(samples_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6669/6669 [00:00<00:00, 76740.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "with open('data/shbank_company_name_sample.txt', 'w', encoding='utf-8') as f:\n",
    "    for idx in tqdm(samples_id):\n",
    "        # if total_data[idx]['text'] in check_name:\n",
    "        #     continue \n",
    "        f.write(json.dumps(total_data[idx], ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "check_name = set()\n",
    "with open('data/name-sample-check.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            l = json.loads(line.strip())\n",
    "            check_name.add(l['text'])\n",
    "        except:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1498"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "with open('data/name-sample-check.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        l = json.loads(line.strip())\n",
    "        samples.append(l)\n",
    "\n",
    "for item in samples:\n",
    "    for k in ['place', 'brand', 'trade', 'suffix']:\n",
    "        if k not in item['label'].keys():\n",
    "            continue\n",
    "        for l in item['label'][k]:\n",
    "            if item['text'][l[1]: l[2]] != l[0]:\n",
    "                print(item['text'], item['text'][l[1]: l[2]], l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('bert-base-chinese/vocab.txt', 'r', encoding='utf-8')\n",
    "words = f.readlines()\n",
    "words_c = set()\n",
    "for w in words:\n",
    "    words_c.add(w.strip())\n",
    "    \n",
    "# for item in samples:\n",
    "#     if len(set(item['text']) - words_c) > 0:\n",
    "#         print(item['text'], set(item['text']) - words_c)\n",
    "#     print(tokenizer.tokenize(item['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import util\n",
    "import torch\n",
    "import config\n",
    "import logging\n",
    "import numpy as np\n",
    "# from data_process import Processor\n",
    "from utils.data_loader import NERDataset\n",
    "from train_file.model import BertNER\n",
    "from train_file.train import train, evaluate\n",
    "from transformers import (\n",
    "  BertTokenizerFast,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup, AdamW\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "batch_size: 64\n"
     ]
    }
   ],
   "source": [
    "filename = config.model_dir\n",
    "batch_size=config.batch_size\n",
    "device = config.device\n",
    "if not os.path.exists(filename):               #判断文件夹是否存在\n",
    "    os.makedirs(filename)                       #新建文件夹\n",
    "\"\"\"train the model\"\"\"\n",
    "# set the logger\n",
    "util.set_logger(config.log_dir)\n",
    "logging.info(\"device: {}\".format(device))\n",
    "logging.info(\"batch_size: {}\".format(batch_size))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class Processor:\n",
    "    def __init__(self, config):\n",
    "        self.data_dir = config.data_dir\n",
    "        self.config = config\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        process train and test data\n",
    "        \"\"\"\n",
    "        for file_name in self.config.files:\n",
    "#             try:\n",
    "            self.preprocess(file_name)\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "    def preprocess(self, mode):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            words：将json文件每一行中的文本分离出来，存储为words列表\n",
    "            labels：标记文本对应的标签，存储为labels\n",
    "        examples:\n",
    "            words示例：['生', '生', '不', '息', 'C', 'S', 'O', 'L']\n",
    "            labels示例：['O', 'O', 'O', 'O', 'B-game', 'I-game', 'I-game', 'I-game']\n",
    "        \"\"\"\n",
    "        bad_tokens = [' ','\\n',' ',' ','​','  ',' �','   ',' 　 ',' � ',' ﻿ ',' \t ',' \t ',' � ','   ',' ﻿ ',' 　 ',' 　 ',\n",
    "                      ' 　 ',' 　 ',' 　 ',' ‍ ',' 　 ','   ','   ',' 　 ',' ﻿ ','   ']\n",
    "        input_dir = self.data_dir + str(mode) + '.json'\n",
    "        output_dir = self.data_dir + str(mode) + '.npz'\n",
    "#         if os.path.exists(output_dir) is True:\n",
    "#             return\n",
    "        word_list = []\n",
    "        label_list = []\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "        with open(input_dir, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                line = line.replace('\\\\xa0','')\n",
    "                # loads()：用于处理内存中的json对象，strip去除可能存在的空格\n",
    "                json_line = json.loads(line.strip())\n",
    "                text = json_line['text']\n",
    "                text = text.lower().replace(\"“\",'\"').replace(\"”\",'\"')\n",
    "                words = tokenizer.tokenize(text)\n",
    "                   \n",
    "                # 如果没有label，则返回None\n",
    "                label_entities = json_line.get('label', None)\n",
    "                labels = ['O'] * len(words)\n",
    "                flag = 0\n",
    "                for key, value in label_entities.items():\n",
    "                    if key == 'symbol':\n",
    "                        continue\n",
    "                    for item in value:\n",
    "                        sub_name = item[0]\n",
    "                        start_index = item[1]\n",
    "                        end_index = item[2]\n",
    "                        # if ''.join(words[item[1]:item[2]]) == sub_name:\n",
    "        #               print(sub_name, ''.join(words[start_index:end_index + 1]))\n",
    "                        flag = 1\n",
    "                        # if len(sub_name) == 1:\n",
    "                        #     labels[start_index] = 'S-' + key\n",
    "                        # else:\n",
    "                        labels[start_index] = 'B-' + key\n",
    "                        labels[start_index + 1:end_index] = ['I-' + key] * (len(sub_name) - 1)\n",
    "                if len(words) < 510 and flag == 1:\n",
    "        #             print(words,labels)\n",
    "                    word_list.append(words)\n",
    "                    label_list.append(labels)\n",
    "\n",
    "            # 保存成二进制文件\n",
    "#         print(word_list,label_list)\n",
    "        np.savez(output_dir, words=np.asanyarray(word_list, dtype=object), \\\n",
    "                 labels=np.asanyarray(label_list, dtype=object), dtype=object)\n",
    "        logging.info(\"--------{} data process DONE!--------\".format(mode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_split(dataset_dir):\n",
    "    \"\"\"split dev set\"\"\"\n",
    "    data = np.load(dataset_dir, allow_pickle=True)\n",
    "    words = data[\"words\"]\n",
    "    labels = data[\"labels\"]\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(words, labels, test_size=config.dev_split_size, random_state=0)\n",
    "    return x_train, x_dev, y_train, y_dev\n",
    "\n",
    "\n",
    "def load_dev(mode):\n",
    "    if mode == 'train':\n",
    "        # 分离出验证集\n",
    "        word_train, word_dev, label_train, label_dev = dev_split(config.train_dir)\n",
    "    elif mode == 'test':\n",
    "        train_data = np.load(config.train_dir, allow_pickle=True)\n",
    "        dev_data = np.load(config.test_dir, allow_pickle=True)\n",
    "        word_train = train_data[\"words\"]\n",
    "        label_train = train_data[\"labels\"]\n",
    "        word_dev = dev_data[\"words\"]\n",
    "        label_dev = dev_data[\"labels\"]\n",
    "    elif mode == 'predict':\n",
    "        train_data = np.load(config.train_dir, allow_pickle=True)\n",
    "        predict_data = np.load(config.predict_dir, allow_pickle=True)\n",
    "        word_train = train_data[\"words\"]\n",
    "        label_train = train_data[\"labels\"]\n",
    "        word_dev = predict_data[\"words\"]\n",
    "        label_dev = predict_data[\"labels\"]\n",
    "    else:\n",
    "        word_train = None\n",
    "        label_train = None\n",
    "        word_dev = None\n",
    "        label_dev = None\n",
    "    return word_train, word_dev, label_train, label_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [00:00, 10426.47it/s]\n",
      "--------name-sample-check data process DONE!--------\n",
      "--------Process Done!--------\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "processor = Processor(config)\n",
    "processor.process()\n",
    "logging.info(\"--------Process Done!--------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------load_dev !--------\n",
      "100%|██████████| 1350/1350 [00:00<00:00, 55290.06it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 55173.79it/s]\n",
      "--------Dataset Build!--------\n"
     ]
    }
   ],
   "source": [
    "# 分离出验证集\n",
    "word_train, word_dev, label_train, label_dev = load_dev('train')\n",
    "logging.info(\"--------load_dev !--------\")\n",
    "# build dataset\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "train_dataset = NERDataset(word_train, label_train, config, tokenizer)\n",
    "dev_dataset = NERDataset(word_dev, label_dev, config, tokenizer)\n",
    "with open(filename+'train_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "with open(filename+'dev_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_dataset, f)\n",
    "logging.info(\"--------Dataset Build!--------\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(label_train)):\n",
    "    if len(word_train[i]) != len(label_train[i]):\n",
    "        print(word_train[i], label_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------Get Dataloader!--------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350\n"
     ]
    }
   ],
   "source": [
    "# get dataset size\n",
    "train_size = len(train_dataset)\n",
    "print(train_size)\n",
    "# build data_loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size,\n",
    "                            shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size,\n",
    "                        shuffle=True, collate_fn=dev_dataset.collate_fn)\n",
    "with open(filename+'train_loader'+str(batch_size)+'.pkl', 'wb') as f:\n",
    "        pickle.dump(train_loader, f)\n",
    "with open(filename+'dev_loader'+str(batch_size)+'.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_loader, f)\n",
    "logging.info(\"--------Get Dataloader!--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertNER were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['bilstm.weight_ih_l1', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l0', 'crf.start_trans', 'classifier.bias', 'bilstm.weight_hh_l1', 'bilstm.weight_hh_l0', 'bilstm.bias_hh_l1_reverse', 'bilstm.bias_ih_l0', 'crf.trans_matrix', 'bilstm.weight_hh_l1_reverse', 'bilstm.bias_ih_l0_reverse', 'crf.end_trans', 'bilstm.weight_ih_l1_reverse', 'bilstm.bias_hh_l1', 'bilstm.bias_ih_l1_reverse', 'bilstm.weight_ih_l0', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'classifier.weight', 'bilstm.weight_ih_l0_reverse']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "--------Create model from bert-base-chinese--------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertNER(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (bilstm): LSTM(768, 384, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       "  (crf): CRF()\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_model_dir = ''\n",
    "# Prepare model\n",
    "if check_model_dir != '':\n",
    "    model = BertNER.from_pretrained(check_model_dir)\n",
    "    logging.info(\"--------Load model from {}--------\".format(check_model_dir))\n",
    "else:\n",
    "    model = BertNER.from_pretrained('bert-base-chinese',num_labels=len(config.label2id))\n",
    "    logging.info(\"--------Create model from {}--------\".format('bert-base-chinese'))\n",
    "model.to(device)\n",
    "# train_loader.to(device)\n",
    "# dev_loader.to(device)\n",
    "# Prepare optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------Start Training!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.13it/s]\n",
      "Epoch: 1, train loss: 1545.6822683160956\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.26it/s]\n",
      "Epoch: 1, dev loss: 1193.7692464192708, f1 score: 0.08695652173913043\n",
      "--------Save best model!--------\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.89it/s]\n",
      "Epoch: 2, train loss: 1350.6452945362437\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.66it/s]\n",
      "Epoch: 2, dev loss: 950.6960042317709, f1 score: 0.304648862512364\n",
      "--------Save best model!--------\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.12it/s]\n",
      "Epoch: 3, train loss: 1050.9796537919478\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.93it/s]\n",
      "Epoch: 3, dev loss: 714.463124593099, f1 score: 0.6486956521739131\n",
      "--------Save best model!--------\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.13it/s]\n",
      "Epoch: 4, train loss: 798.6135052767667\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.01it/s]\n",
      "Epoch: 4, dev loss: 548.5558624267578, f1 score: 0.8295652173913043\n",
      "--------Save best model!--------\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.14it/s]\n",
      "Epoch: 5, train loss: 616.2562056454746\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.13it/s]\n",
      "Epoch: 5, dev loss: 425.7250264485677, f1 score: 0.8442123585726719\n",
      "--------Save best model!--------\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.07it/s]\n",
      "Epoch: 6, train loss: 476.4180599559437\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.97it/s]\n",
      "Epoch: 6, dev loss: 334.0990498860677, f1 score: 0.8821490467937609\n",
      "--------Save best model!--------\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.21it/s]\n",
      "Epoch: 7, train loss: 367.44595952467483\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.76it/s]\n",
      "Epoch: 7, dev loss: 262.91315205891925, f1 score: 0.8909871244635192\n",
      "--------Save best model!--------\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.31it/s]\n",
      "Epoch: 8, train loss: 280.8219328793612\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.99it/s]\n",
      "Epoch: 8, dev loss: 215.13970438639322, f1 score: 0.9015544041450777\n",
      "--------Save best model!--------\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.17it/s]\n",
      "Epoch: 9, train loss: 217.95468469099566\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.36it/s]\n",
      "Epoch: 9, dev loss: 173.17449188232422, f1 score: 0.9194805194805195\n",
      "--------Save best model!--------\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.95it/s]\n",
      "Epoch: 10, train loss: 168.03751321272418\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.46it/s]\n",
      "Epoch: 10, dev loss: 152.7171376546224, f1 score: 0.9006050129645635\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.97it/s]\n",
      "Epoch: 11, train loss: 135.94511777704412\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.24it/s]\n",
      "Epoch: 11, dev loss: 138.46925354003906, f1 score: 0.909404659188956\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.89it/s]\n",
      "Epoch: 12, train loss: 110.91389153220437\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.03it/s]\n",
      "Epoch: 12, dev loss: 133.9002456665039, f1 score: 0.9213483146067416\n",
      "--------Save best model!--------\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.03it/s]\n",
      "Epoch: 13, train loss: 89.22043124112216\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.07it/s]\n",
      "Epoch: 13, dev loss: 123.40850830078125, f1 score: 0.9225473321858864\n",
      "--------Save best model!--------\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.07it/s]\n",
      "Epoch: 14, train loss: 74.86345152421431\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.25it/s]\n",
      "Epoch: 14, dev loss: 123.4172592163086, f1 score: 0.9069171648163962\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.39it/s]\n",
      "Epoch: 15, train loss: 67.39209175109863\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.71it/s]\n",
      "Epoch: 15, dev loss: 117.09199523925781, f1 score: 0.9176672384219554\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.19it/s]\n",
      "Epoch: 16, train loss: 55.62722102078524\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.16it/s]\n",
      "Epoch: 16, dev loss: 110.57240549723308, f1 score: 0.9075487701441901\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.08it/s]\n",
      "Epoch: 17, train loss: 44.29355205189098\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.70it/s]\n",
      "Epoch: 17, dev loss: 124.3110860188802, f1 score: 0.9147286821705425\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.34it/s]\n",
      "Epoch: 18, train loss: 35.837087804620914\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.27it/s]\n",
      "Epoch: 18, dev loss: 113.77519861857097, f1 score: 0.9234737747205504\n",
      "--------Save best model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.99it/s]\n",
      "Epoch: 19, train loss: 34.44332486932928\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.73it/s]\n",
      "Epoch: 19, dev loss: 129.559326171875, f1 score: 0.9128205128205129\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.51it/s]\n",
      "Epoch: 20, train loss: 27.88669378107244\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.68it/s]\n",
      "Epoch: 20, dev loss: 96.59896723429362, f1 score: 0.9213863060016907\n",
      "--------Save best loss model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.91it/s]\n",
      "Epoch: 21, train loss: 24.177034291354094\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.92it/s]\n",
      "Epoch: 21, dev loss: 106.21476236979167, f1 score: 0.9294817332200509\n",
      "--------Save best model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.95it/s]\n",
      "Epoch: 22, train loss: 18.470217097889293\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.14it/s]\n",
      "Epoch: 22, dev loss: 121.60896809895833, f1 score: 0.9118644067796611\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.39it/s]\n",
      "Epoch: 23, train loss: 17.594662059437145\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.04it/s]\n",
      "Epoch: 23, dev loss: 129.22438049316406, f1 score: 0.9242553191489361\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.65it/s]\n",
      "Epoch: 24, train loss: 13.787214582616633\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.75it/s]\n",
      "Epoch: 24, dev loss: 128.5942815144857, f1 score: 0.9249146757679181\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.57it/s]\n",
      "Epoch: 25, train loss: 11.859098651192404\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.71it/s]\n",
      "Epoch: 25, dev loss: 133.16189829508463, f1 score: 0.9254498714652957\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.29it/s]\n",
      "Epoch: 26, train loss: 13.973254940726541\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.74it/s]\n",
      "Epoch: 26, dev loss: 166.18664042154947, f1 score: 0.9205807002561913\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.37it/s]\n",
      "Epoch: 27, train loss: 10.548742207613858\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.92it/s]\n",
      "Epoch: 27, dev loss: 135.6996841430664, f1 score: 0.9212328767123289\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.41it/s]\n",
      "Epoch: 28, train loss: 8.558789166537197\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.63it/s]\n",
      "Epoch: 28, dev loss: 142.06701278686523, f1 score: 0.9272882805816938\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.26it/s]\n",
      "Epoch: 29, train loss: 5.964500470594927\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.92it/s]\n",
      "Epoch: 29, dev loss: 143.25045267740884, f1 score: 0.9151103565365025\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.49it/s]\n",
      "Epoch: 30, train loss: 5.117864348671653\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.77it/s]\n",
      "Epoch: 30, dev loss: 142.41204325358072, f1 score: 0.9305912596401028\n",
      "--------Save best model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.04it/s]\n",
      "Epoch: 31, train loss: 3.651734958995472\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.79it/s]\n",
      "Epoch: 31, dev loss: 158.17925008138022, f1 score: 0.9339055793991415\n",
      "--------Save best model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.03it/s]\n",
      "Epoch: 32, train loss: 3.9216977032748135\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.36it/s]\n",
      "Epoch: 32, dev loss: 160.31965128580728, f1 score: 0.9216354344122659\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.47it/s]\n",
      "Epoch: 33, train loss: 1.8592273105274548\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.89it/s]\n",
      "Epoch: 33, dev loss: 164.71344121297201, f1 score: 0.9332191780821918\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.38it/s]\n",
      "Epoch: 34, train loss: 1.7033425244418057\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.87it/s]\n",
      "Epoch: 34, dev loss: 169.6979522705078, f1 score: 0.9308283518360376\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.33it/s]\n",
      "Epoch: 35, train loss: 2.6783928871154785\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.21it/s]\n",
      "Epoch: 35, dev loss: 173.68955103556314, f1 score: 0.9357326478149101\n",
      "--------Save best model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.05it/s]\n",
      "Epoch: 36, train loss: 2.0257380225441675\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.75it/s]\n",
      "Epoch: 36, dev loss: 186.73235829671225, f1 score: 0.9188727583262168\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.40it/s]\n",
      "Epoch: 37, train loss: 0.9833797975019976\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.62it/s]\n",
      "Epoch: 37, dev loss: 171.6758804321289, f1 score: 0.9258312020460359\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.35it/s]\n",
      "Epoch: 38, train loss: 0.45468165657737036\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.08it/s]\n",
      "Epoch: 38, dev loss: 178.7188720703125, f1 score: 0.9382504288164665\n",
      "--------Save best model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.99it/s]\n",
      "Epoch: 39, train loss: 0.4476540305397727\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.73it/s]\n",
      "Epoch: 39, dev loss: 188.76612345377603, f1 score: 0.9272882805816938\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.48it/s]\n",
      "Epoch: 40, train loss: 0.3485540043223988\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.98it/s]\n",
      "Epoch: 40, dev loss: 183.62342071533203, f1 score: 0.9398625429553265\n",
      "--------Save best model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.96it/s]\n",
      "Epoch: 41, train loss: 0.27446937561035156\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.65it/s]\n",
      "Epoch: 41, dev loss: 189.92919921875, f1 score: 0.9307100085543198\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.48it/s]\n",
      "Epoch: 42, train loss: 0.2495394620028409\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.95it/s]\n",
      "Epoch: 42, dev loss: 198.88684844970703, f1 score: 0.9381443298969072\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.53it/s]\n",
      "Epoch: 43, train loss: 1.192373969338157\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.71it/s]\n",
      "Epoch: 43, dev loss: 182.21359252929688, f1 score: 0.9443016281062554\n",
      "--------Save best model!--------\n",
      "100%|██████████| 22/22 [00:02<00:00,  9.98it/s]\n",
      "Epoch: 44, train loss: 1.1941289034756748\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.01it/s]\n",
      "Epoch: 44, dev loss: 202.23489634195963, f1 score: 0.9356223175965664\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.24it/s]\n",
      "Epoch: 45, train loss: 0.18281017650257458\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.88it/s]\n",
      "Epoch: 45, dev loss: 208.73983256022134, f1 score: 0.9266211604095562\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.39it/s]\n",
      "Epoch: 46, train loss: 0.9665817780928179\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.87it/s]\n",
      "Epoch: 46, dev loss: 214.97445170084634, f1 score: 0.9332191780821918\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.40it/s]\n",
      "Epoch: 47, train loss: 0.33788403597745026\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.92it/s]\n",
      "Epoch: 47, dev loss: 193.49452209472656, f1 score: 0.9366438356164383\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.21it/s]\n",
      "Epoch: 48, train loss: 1.2729825106534092\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.75it/s]\n",
      "Epoch: 48, dev loss: 213.9430185953776, f1 score: 0.9326513213981245\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.51it/s]\n",
      "Epoch: 49, train loss: 0.2606948505748402\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.08it/s]\n",
      "Epoch: 49, dev loss: 213.51094818115234, f1 score: 0.932420872540633\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.41it/s]\n",
      "Epoch: 50, train loss: 0.29355569319291547\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.80it/s]\n",
      "Epoch: 50, dev loss: 204.30746459960938, f1 score: 0.9359521776259607\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.32it/s]\n",
      "Epoch: 51, train loss: 0.1149186221036044\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.84it/s]\n",
      "Epoch: 51, dev loss: 197.92474873860678, f1 score: 0.9384615384615385\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.43it/s]\n",
      "Epoch: 52, train loss: 0.10924590717662465\n",
      "100%|██████████| 3/3 [00:00<00:00, 16.80it/s]\n",
      "Epoch: 52, dev loss: 199.6935068766276, f1 score: 0.9384615384615385\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.26it/s]\n",
      "Epoch: 53, train loss: 0.1898112730546431\n",
      "100%|██████████| 3/3 [00:00<00:00, 17.16it/s]\n",
      "Epoch: 53, dev loss: 218.6027806599935, f1 score: 0.9283276450511946\n",
      "Best val f1: 0.9443016281062554\n",
      "Training Finished!\n"
     ]
    }
   ],
   "source": [
    "if config.full_fine_tuning:\n",
    "    # model.named_parameters(): [bert, bilstm, classifier, crf]\n",
    "    bert_optimizer = list(model.bert.named_parameters())\n",
    "    # lstm_optimizer = list(model.bilstm.named_parameters())\n",
    "    classifier_optimizer = list(model.classifier.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in bert_optimizer if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': config.weight_decay},\n",
    "        {'params': [p for n, p in bert_optimizer if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.0},\n",
    "        # {'params': [p for n, p in lstm_optimizer if not any(nd in n for nd in no_decay)],\n",
    "            # 'lr': config.learning_rate * 20, 'weight_decay': config.weight_decay},\n",
    "        # {'params': [p for n, p in lstm_optimizer if any(nd in n for nd in no_decay)],\n",
    "            # 'lr': config.learning_rate * 20, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in classifier_optimizer if not any(nd in n for nd in no_decay)],\n",
    "            'lr': config.learning_rate * 20, 'weight_decay': config.weight_decay},\n",
    "        {'params': [p for n, p in classifier_optimizer if any(nd in n for nd in no_decay)],\n",
    "            'lr': config.learning_rate * 20, 'weight_decay': 0.0},\n",
    "        {'params': model.crf.parameters(), 'lr': config.learning_rate * 20}\n",
    "    ]\n",
    "# only fine-tune the head classifier\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate, correct_bias=False)\n",
    "\n",
    "train_steps_per_epoch = train_size // config.batch_size\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=(config.epoch_num // 10) * train_steps_per_epoch,\n",
    "                                            num_training_steps=config.epoch_num * train_steps_per_epoch)\n",
    "# Train the model\n",
    "logging.info(\"--------Start Training!--------\")\n",
    "train(train_loader, dev_loader, model, optimizer, scheduler, config.model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, config.model_dir + 'last_model/' + 'model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 12.84it/s]\n",
      "--------Predict finished !--------\n"
     ]
    }
   ],
   "source": [
    "from utils.metrics import f1_score, bad_case, predict_output, predict_output_pre\n",
    "# def evaluate_pre(dev_loader, model, tokenizer,mode='dev',flag=1):\n",
    "# set model to evaluation mode\n",
    "mode = 'predict'\n",
    "model.eval()\n",
    "id2label = config.id2label\n",
    "true_tags = []\n",
    "pred_tags = []\n",
    "sent_data = []\n",
    "dev_losses = 0\n",
    "len_dev_loader = 0\n",
    "with torch.no_grad():\n",
    "    for idx, batch_samples in enumerate(tqdm(dev_loader)):\n",
    "        batch_data, batch_token_starts, batch_tags = batch_samples\n",
    "        if mode == 'test' or mode == 'predict':\n",
    "            sent_data.extend([[tokenizer.convert_ids_to_tokens(idx.item()) for idx in indices\n",
    "                               if (idx.item() > 0 and idx.item() != 101)] for indices in batch_data])\n",
    "        batch_masks = batch_data.gt(0)  # get padding mask, gt(x): get index greater than x\n",
    "        label_masks = batch_tags.gt(-1)  # get padding mask, gt(x): get index greater than x\n",
    "        len_dev_loader += 1\n",
    "        # (batch_size, max_len, num_labels)\n",
    "        batch_output = model((batch_data, batch_token_starts),\n",
    "                             token_type_ids=None, attention_mask=batch_masks)[0]\n",
    "        # (batch_size, max_len - padding_label_len)\n",
    "        batch_output = model.crf.viterbi_decode(batch_output, mask=label_masks)\n",
    "        # (batch_size, max_len)\n",
    "        batch_tags = batch_tags.to('cpu').numpy()\n",
    "        pred_tags.extend([[id2label.get(idx) for idx in indices] for indices in batch_output])\n",
    "        # (batch_size, max_len - padding_label_len)\n",
    "        true_tags.extend([[id2label.get(idx) for idx in indices if idx > -1] for indices in batch_tags])\n",
    "\n",
    "# print(len(pred_tags), len(true_tags))\n",
    "# print(pred_tags, true_tags)\n",
    "assert len(pred_tags) == len(true_tags)\n",
    "if mode == 'test' or mode == 'predict':\n",
    "    assert len(sent_data) == len(true_tags)\n",
    "\n",
    "# logging loss, f1 and report\n",
    "metrics = {}\n",
    "\n",
    "if mode == 'predict' or mode == 'test':\n",
    "    # print(pred_tags,sent_data)\n",
    "    all_end=predict_output_pre(pred_tags, sent_data)\n",
    "    \n",
    "metrics['loss'] = float(dev_losses) / len_dev_loader\n",
    "#     if flag==0:\n",
    "#         return metrics,all_end\n",
    "#     else:\n",
    "#         return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['灌南苏盛能源有限公司',\n",
       "  {'place': [['灌南', 0, 2]],\n",
       "   'brand': [['苏盛', 2, 4]],\n",
       "   'trade': [['能源', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['康达医疗康复设备有限公司',\n",
       "  {'brand': [['康达', 0, 2]],\n",
       "   'trade': [['医疗', 2, 4], ['康复设备', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['固原市第四建筑工程公司',\n",
       "  {'place': [['固原市', 0, 3]],\n",
       "   'brand': [['第四', 3, 5]],\n",
       "   'trade': [['建筑工程', 5, 9]],\n",
       "   'suffix': [['公司', 9, 11]]}],\n",
       " ['长沙新兴服务劳务公司',\n",
       "  {'place': [['长沙', 0, 2]],\n",
       "   'brand': [['新兴', 2, 4]],\n",
       "   'trade': [['服务劳务', 4, 8]],\n",
       "   'suffix': [['公司', 8, 10]]}],\n",
       " ['四川新能源开发公司',\n",
       "  {'place': [['四川', 0, 2]],\n",
       "   'trade': [['新能源开发', 2, 7]],\n",
       "   'suffix': [['公司', 7, 9]]}],\n",
       " ['昆山正亚电子有限公司',\n",
       "  {'place': [['昆山', 0, 2]],\n",
       "   'brand': [['正亚', 2, 4]],\n",
       "   'trade': [['电子', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['成都天立化工科学有限公司',\n",
       "  {'place': [['成都', 0, 2]],\n",
       "   'brand': [['天立', 2, 4]],\n",
       "   'trade': [['化工科学', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['南京平安总公司',\n",
       "  {'place': [['南京', 0, 2]],\n",
       "   'brand': [['平安', 2, 4]],\n",
       "   'suffix': [['总公司', 4, 7]]}],\n",
       " ['云南农垦电力公司',\n",
       "  {'place': [['云南', 0, 2]],\n",
       "   'trade': [['农垦', 2, 4], ['电力', 4, 6]],\n",
       "   'suffix': [['公司', 6, 8]]}],\n",
       " ['深圳厚存京英企业管理合伙企业（有限合伙）',\n",
       "  {'place': [['深圳', 0, 2]],\n",
       "   'brand': [['厚存京英', 2, 6]],\n",
       "   'trade': [['企业管理', 6, 10]],\n",
       "   'suffix': [['合伙企业', 10, 14], ['有限合伙', 15, 19]]}],\n",
       " ['北京中城融丰资产管理有限公司',\n",
       "  {'place': [['北京', 0, 2]],\n",
       "   'brand': [['中城融丰', 2, 6]],\n",
       "   'trade': [['资产管理', 6, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['凤阳健民蓝莓农业发展有限公司',\n",
       "  {'place': [['凤阳', 0, 2]],\n",
       "   'brand': [['健民', 2, 4]],\n",
       "   'trade': [['蓝莓', 4, 6], ['农业发展', 6, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['武汉市丽华园酒店管理有限公司',\n",
       "  {'place': [['武汉市', 0, 3]],\n",
       "   'brand': [['丽华园', 3, 6]],\n",
       "   'trade': [['酒店管理', 6, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['广西兰庭房地产经纪有限公司',\n",
       "  {'place': [['广西', 0, 2]],\n",
       "   'brand': [['兰庭', 2, 4]],\n",
       "   'trade': [['房地产经纪', 4, 9]],\n",
       "   'suffix': [['有限公司', 9, 13]]}],\n",
       " ['合肥网站设计公司',\n",
       "  {'place': [['合肥', 0, 2]],\n",
       "   'trade': [['网站设计', 2, 6]],\n",
       "   'suffix': [['公司', 6, 8]]}],\n",
       " ['满记甜品港币资本金活期', {'brand': [['满记', 0, 2]], 'trade': [['甜品', 2, 4]]}],\n",
       " ['华能贵诚信托有限公司',\n",
       "  {'brand': [['华能贵诚', 0, 4]],\n",
       "   'trade': [['信托', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['杭州唯一织造厂',\n",
       "  {'place': [['杭州', 0, 2]],\n",
       "   'brand': [['唯一', 2, 4]],\n",
       "   'trade': [['织造', 4, 6]],\n",
       "   'suffix': [['厂', 6, 7]]}],\n",
       " ['厦门宏润实业发展有限公司',\n",
       "  {'place': [['厦门', 0, 2]],\n",
       "   'brand': [['宏润', 2, 4]],\n",
       "   'trade': [['实业发展', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['浙江捷胜化学工业有限公司',\n",
       "  {'place': [['浙江', 0, 2]],\n",
       "   'brand': [['捷胜', 2, 4]],\n",
       "   'trade': [['化学工业', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['成都守正智慧投资咨询有限公司',\n",
       "  {'place': [['成都', 0, 2]],\n",
       "   'brand': [['守正', 2, 4]],\n",
       "   'trade': [['智慧投资咨询', 4, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['浙江洁宝精工机械有限公司',\n",
       "  {'place': [['浙江', 0, 2]],\n",
       "   'brand': [['洁宝', 2, 4]],\n",
       "   'trade': [['精工机械', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['荣庆保安管理有限公司',\n",
       "  {'brand': [['荣庆', 0, 2]],\n",
       "   'trade': [['保安管理', 2, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['深圳市中科电实业发展有限公司',\n",
       "  {'place': [['深圳市', 0, 3]],\n",
       "   'brand': [['中科电', 3, 6]],\n",
       "   'trade': [['实业发展', 6, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['上海玄设计装潢工程有限公司',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['玄', 2, 4]],\n",
       "   'trade': [['设计装潢工程', 4, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['沈阳建筑幕墙公司',\n",
       "  {'place': [['沈阳', 0, 2]],\n",
       "   'trade': [['建筑幕墙', 2, 6]],\n",
       "   'suffix': [['公司', 6, 8]]}],\n",
       " ['胡适为保险公司',\n",
       "  {'brand': [['胡适', 0, 2]],\n",
       "   'trade': [['为', 2, 3], ['保险', 3, 5]],\n",
       "   'suffix': [['公司', 5, 7]]}],\n",
       " ['重庆批发酒水公司',\n",
       "  {'place': [['重庆', 0, 2]],\n",
       "   'trade': [['批发酒水', 2, 6]],\n",
       "   'suffix': [['公司', 6, 8]]}],\n",
       " ['嘉兴市南湖投资开发建设集团有限公司',\n",
       "  {'place': [['嘉兴市', 0, 3]],\n",
       "   'brand': [['南湖', 3, 5]],\n",
       "   'trade': [['投资开发建设', 5, 11]],\n",
       "   'suffix': [['集团有限公司', 11, 17]]}],\n",
       " ['上海太平洋人寿保险股份有限公司',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['太平洋', 2, 5]],\n",
       "   'trade': [['人寿保险', 5, 9]],\n",
       "   'suffix': [['股份有限公司', 9, 15]]}],\n",
       " ['山西地煤电力设备有限公司',\n",
       "  {'place': [['山西', 0, 2]],\n",
       "   'brand': [['地煤', 2, 4]],\n",
       "   'trade': [['电力设备', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['剑门蜀门电子商贸有限公司',\n",
       "  {'place': [['剑门', 0, 2]],\n",
       "   'brand': [['蜀门', 2, 4]],\n",
       "   'trade': [['电子商贸', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['上海瑞浩装饰设计有限公司',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['瑞浩', 2, 4]],\n",
       "   'trade': [['装饰设计', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['合肥正捷智能科技股份有限公司',\n",
       "  {'place': [['合肥', 0, 2]],\n",
       "   'brand': [['正捷', 2, 4]],\n",
       "   'trade': [['智能科技', 4, 8]],\n",
       "   'suffix': [['股份有限公司', 8, 14]]}],\n",
       " ['江苏南通飞天化学实业有限公司',\n",
       "  {'place': [['江苏南通', 0, 4]],\n",
       "   'brand': [['飞天', 4, 6]],\n",
       "   'trade': [['化学实业', 6, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['中铁广盛桥梁建筑工程有限公司',\n",
       "  {'brand': [['中铁广盛', 0, 4]],\n",
       "   'trade': [['桥梁', 4, 6], ['建筑工程', 6, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['深圳车仆汽车用品发展有限公司',\n",
       "  {'place': [['深圳', 0, 2]],\n",
       "   'brand': [['车仆', 2, 4]],\n",
       "   'trade': [['汽车用品发展', 4, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['北京响米傲业商贸有限公司',\n",
       "  {'place': [['北京', 0, 2]],\n",
       "   'brand': [['响米傲业', 2, 6]],\n",
       "   'trade': [['商贸', 6, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['珀挺机械工业厦门有限公司',\n",
       "  {'brand': [['珀挺', 0, 2]],\n",
       "   'trade': [['机械工业', 2, 6]],\n",
       "   'place': [['厦门', 6, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['福州仁山农业科技有限公司',\n",
       "  {'place': [['福州', 0, 2]],\n",
       "   'brand': [['仁山', 2, 4]],\n",
       "   'trade': [['农业科技', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['盛华景酒店管理公司',\n",
       "  {'brand': [['盛华景', 0, 3]],\n",
       "   'trade': [['酒店管理', 3, 7]],\n",
       "   'suffix': [['公司', 7, 9]]}],\n",
       " ['广东中广信资产评估机构分公司',\n",
       "  {'place': [['广东', 0, 2]],\n",
       "   'brand': [['中广信', 2, 5]],\n",
       "   'trade': [['资产评估', 5, 9]],\n",
       "   'suffix': [['机构', 9, 11], ['分公司', 11, 14]]}],\n",
       " ['芒夏科技（上海）有限公司',\n",
       "  {'brand': [['芒夏', 0, 2]],\n",
       "   'trade': [['科技', 2, 4]],\n",
       "   'place': [['上海', 5, 7]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['宁波瑞庭投资有限公司',\n",
       "  {'place': [['宁波', 0, 2]],\n",
       "   'brand': [['瑞庭', 2, 4]],\n",
       "   'trade': [['投资', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['东顺装饰', {'brand': [['东顺', 0, 2]], 'trade': [['装饰', 2, 4]]}],\n",
       " ['四川望风青苹果纸业有限公司',\n",
       "  {'place': [['四川', 0, 2]],\n",
       "   'brand': [['望风青', 2, 5]],\n",
       "   'trade': [['苹果纸业', 5, 9]],\n",
       "   'suffix': [['有限公司', 9, 13]]}],\n",
       " ['瑞锦投资咨询有限公司',\n",
       "  {'brand': [['瑞锦', 0, 2]],\n",
       "   'trade': [['投资咨询', 2, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['中山市美迪达电器有限公司',\n",
       "  {'place': [['中山市', 0, 3]],\n",
       "   'brand': [['美迪达', 3, 6]],\n",
       "   'trade': [['电器', 6, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['昆明万达铁路运输公司',\n",
       "  {'place': [['昆明', 0, 2]],\n",
       "   'brand': [['万达', 2, 4]],\n",
       "   'trade': [['铁路运输', 4, 8]],\n",
       "   'suffix': [['公司', 8, 10]]}],\n",
       " ['南宁远东电子商务有限公司',\n",
       "  {'place': [['南宁', 0, 2]],\n",
       "   'brand': [['远东', 2, 4]],\n",
       "   'trade': [['电子商务', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['海南乐游动漫有限公司',\n",
       "  {'place': [['海南', 0, 2]],\n",
       "   'brand': [['乐游', 2, 4]],\n",
       "   'trade': [['动漫', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['中山汉企教育网络科技有限公司',\n",
       "  {'place': [['中山', 0, 2]],\n",
       "   'brand': [['汉企', 2, 4]],\n",
       "   'trade': [['教育网络科技', 4, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['南京赢德企业管理有限公司',\n",
       "  {'place': [['南京', 0, 2]],\n",
       "   'brand': [['赢德', 2, 4]],\n",
       "   'trade': [['企业管理', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['河北凯普达汽车部件制造有限公司',\n",
       "  {'place': [['河北', 0, 2]],\n",
       "   'brand': [['凯普达', 2, 5]],\n",
       "   'trade': [['汽车部件制造', 5, 11]],\n",
       "   'suffix': [['有限公司', 11, 15]]}],\n",
       " ['上海人寿保险股份有限公司万能险上行托管',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'trade': [['人寿保险', 2, 6], ['万能险', 12, 15], ['托管', 17, 19]],\n",
       "   'suffix': [['股份有限公司', 6, 12]],\n",
       "   'brand': [['上行', 15, 17]]}],\n",
       " ['东莞市佳旭钢材贸易有限公司',\n",
       "  {'place': [['东莞市', 0, 3]],\n",
       "   'brand': [['佳旭', 3, 5]],\n",
       "   'trade': [['钢材贸易', 5, 9]],\n",
       "   'suffix': [['有限公司', 9, 13]]}],\n",
       " ['大连兴旺船舶修理有限公司',\n",
       "  {'place': [['大连', 0, 2]],\n",
       "   'brand': [['兴旺', 2, 4]],\n",
       "   'trade': [['船舶修理', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['重庆国际技术合作公司',\n",
       "  {'place': [['重庆', 0, 2]],\n",
       "   'trade': [['国际技术合作', 2, 8]],\n",
       "   'suffix': [['公司', 8, 10]]}],\n",
       " ['华锻（安徽）机床制造有限公司',\n",
       "  {'brand': [['华锻', 0, 2]],\n",
       "   'place': [['安徽', 3, 5]],\n",
       "   'trade': [['机床制造', 6, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['河北馆陶农资公司',\n",
       "  {'place': [['河北', 0, 2]],\n",
       "   'brand': [['馆陶', 2, 4]],\n",
       "   'trade': [['农资', 4, 6]],\n",
       "   'suffix': [['公司', 6, 8]]}],\n",
       " ['中国石油天然气销售河北分公司',\n",
       "  {'place': [['中国', 0, 2], ['河北', 9, 11]],\n",
       "   'trade': [['石油', 2, 4], ['天然气销售', 4, 9]],\n",
       "   'suffix': [['分公司', 11, 14]]}],\n",
       " ['广州百润捷供应链有限公司',\n",
       "  {'place': [['广州', 0, 2]],\n",
       "   'brand': [['百润捷', 2, 5]],\n",
       "   'trade': [['供应链', 5, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['浙江兽王皮业服饰有限公司',\n",
       "  {'place': [['浙江', 0, 2]],\n",
       "   'brand': [['兽王', 2, 4]],\n",
       "   'trade': [['皮业', 4, 6], ['服饰', 6, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['杭州汇淘电子商务有限公司',\n",
       "  {'place': [['杭州', 0, 2]],\n",
       "   'brand': [['汇淘', 2, 4]],\n",
       "   'trade': [['电子商务', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['隆（上海）钢材贸易有限公司资本金',\n",
       "  {'brand': [['隆', 0, 2]],\n",
       "   'place': [['上海', 3, 5]],\n",
       "   'trade': [['钢材贸易', 6, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['上海汇祥建材商行',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['汇祥', 2, 4]],\n",
       "   'trade': [['建材', 4, 6]],\n",
       "   'suffix': [['商行', 6, 8]]}],\n",
       " ['成都城北医院有限公司',\n",
       "  {'place': [['成都', 0, 2]],\n",
       "   'brand': [['城北', 2, 4]],\n",
       "   'trade': [['医院', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['北京速汇广告有限公司',\n",
       "  {'place': [['北京', 0, 2]],\n",
       "   'brand': [['速汇', 2, 4]],\n",
       "   'trade': [['广告', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['郑州六顺劳务分包有限公司',\n",
       "  {'place': [['郑州', 0, 2]],\n",
       "   'brand': [['六顺', 2, 4]],\n",
       "   'trade': [['劳务分包', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['威海木业机械有限公司',\n",
       "  {'place': [['威海', 0, 2]],\n",
       "   'trade': [['木业机械', 2, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['洛阳豫方不动产评估测绘有限公司',\n",
       "  {'place': [['洛阳', 0, 2]],\n",
       "   'brand': [['豫方', 2, 4]],\n",
       "   'trade': [['不动产评估测绘', 4, 11]],\n",
       "   'suffix': [['有限公司', 11, 15]]}],\n",
       " ['东吴大学上海校友会',\n",
       "  {'brand': [['东吴', 0, 2]],\n",
       "   'trade': [['大学', 2, 4], ['校友会', 6, 9]],\n",
       "   'place': [['上海', 4, 6]]}],\n",
       " ['宁夏京成天宝科技有限公司',\n",
       "  {'place': [['宁夏', 0, 2]],\n",
       "   'brand': [['京成天宝', 2, 6]],\n",
       "   'trade': [['科技', 6, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['山东信发木业有限公司',\n",
       "  {'place': [['山东', 0, 2]],\n",
       "   'brand': [['信发', 2, 4]],\n",
       "   'trade': [['木业', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['沈阳华润置地物业服务有限公司',\n",
       "  {'place': [['沈阳', 0, 2]],\n",
       "   'brand': [['华润', 2, 4]],\n",
       "   'trade': [['置地', 4, 6], ['物业服务', 6, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['瑞德威进出口贸易有限公司',\n",
       "  {'brand': [['瑞德威', 0, 3]],\n",
       "   'trade': [['进出口贸易', 3, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['广州益力多公司',\n",
       "  {'place': [['广州', 0, 2]],\n",
       "   'brand': [['益力多', 2, 5]],\n",
       "   'suffix': [['公司', 5, 7]]}],\n",
       " ['武汉南洋美佳装饰公司',\n",
       "  {'place': [['武汉', 0, 2]],\n",
       "   'brand': [['南洋美佳', 2, 6]],\n",
       "   'trade': [['装饰', 6, 8]],\n",
       "   'suffix': [['公司', 8, 10]]}],\n",
       " ['安徽银泰餐饮有限公司',\n",
       "  {'place': [['安徽', 0, 2]],\n",
       "   'brand': [['银泰', 2, 4]],\n",
       "   'trade': [['餐饮', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['大连维恩医疗美容诊所有限公司',\n",
       "  {'place': [['大连', 0, 2]],\n",
       "   'brand': [['维恩', 2, 4]],\n",
       "   'trade': [['医疗美容', 4, 8], ['诊所', 8, 10]],\n",
       "   'suffix': [['有限公司', 10, 14]]}],\n",
       " ['天津制冷技术有限公司',\n",
       "  {'place': [['天津', 0, 2]],\n",
       "   'trade': [['制冷技术', 2, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['河南建设开发有限公司',\n",
       "  {'place': [['河南', 0, 2]],\n",
       "   'trade': [['建设开发', 2, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['合肥亦诚管理咨询有限公司',\n",
       "  {'place': [['合肥', 0, 2]],\n",
       "   'brand': [['亦诚', 2, 4]],\n",
       "   'trade': [['管理咨询', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['中国生物科技有限公司',\n",
       "  {'place': [['中国', 0, 2]],\n",
       "   'trade': [['生物科技', 2, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['吉达建材设备有限公司',\n",
       "  {'brand': [['吉达', 0, 2]],\n",
       "   'trade': [['建材设备', 2, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['东莞尊创五金制品有限公司',\n",
       "  {'place': [['东莞', 0, 2]],\n",
       "   'brand': [['尊创', 2, 4]],\n",
       "   'trade': [['五金制品', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['永丰金融资租赁(天津)有限公司',\n",
       "  {'brand': [['永丰', 0, 2]],\n",
       "   'trade': [['金融资租赁', 2, 7]],\n",
       "   'place': [['天津', 8, 10]],\n",
       "   'suffix': [['有限公司', 11, 15]]}],\n",
       " ['上海东方风机实业有限公司',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['东方', 2, 4]],\n",
       "   'trade': [['风机实业', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['梅州市建筑设计有限公司',\n",
       "  {'place': [['梅州市', 0, 3]],\n",
       "   'trade': [['建筑设计', 3, 7]],\n",
       "   'suffix': [['有限公司', 7, 11]]}],\n",
       " ['河南省新郑电力实业总公司',\n",
       "  {'place': [['河南省', 0, 3]],\n",
       "   'brand': [['新郑', 3, 5]],\n",
       "   'trade': [['电力实业', 5, 9]],\n",
       "   'suffix': [['总公司', 9, 12]]}],\n",
       " ['上海普运电子商行',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['普运', 2, 4]],\n",
       "   'trade': [['电子商行', 4, 8]]}],\n",
       " ['湖北仝鑫工贸股份有限公司',\n",
       "  {'place': [['湖北', 0, 2]],\n",
       "   'brand': [['仝鑫', 2, 4]],\n",
       "   'trade': [['工贸', 4, 6]],\n",
       "   'suffix': [['股份有限公司', 6, 12]]}],\n",
       " ['永仁资产管理公司',\n",
       "  {'brand': [['永仁', 0, 2]],\n",
       "   'trade': [['资产管理', 2, 6]],\n",
       "   'suffix': [['公司', 6, 8]]}],\n",
       " ['广州一本机械有限公司',\n",
       "  {'place': [['广州', 0, 2]],\n",
       "   'brand': [['一本', 2, 4]],\n",
       "   'trade': [['机械', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['湖南闽益农业开发有限公司',\n",
       "  {'place': [['湖南', 0, 2]],\n",
       "   'brand': [['闽益', 2, 4]],\n",
       "   'trade': [['农业开发', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['英程国际教育咨询有限公司',\n",
       "  {'brand': [['英程', 0, 2]],\n",
       "   'trade': [['国际教育咨询', 2, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['世坤咨询北京有限公司',\n",
       "  {'brand': [['世坤', 0, 2]],\n",
       "   'trade': [['咨询', 2, 4]],\n",
       "   'place': [['北京', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['上海立翔昌兴业进出口贸易有限公司',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['立翔昌', 2, 5]],\n",
       "   'trade': [['兴业', 5, 7], ['进出口贸易', 7, 12]],\n",
       "   'suffix': [['有限公司', 12, 16]]}],\n",
       " ['台州鑫辉光电有限公司',\n",
       "  {'place': [['台州', 0, 2]],\n",
       "   'brand': [['鑫辉', 2, 4]],\n",
       "   'trade': [['光电', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['上海柯帕机电设备有限公司',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['柯帕', 2, 4]],\n",
       "   'trade': [['机电设备', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['河北交通设施工程有限公司',\n",
       "  {'place': [['河北', 0, 2]],\n",
       "   'trade': [['交通设施工程', 2, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['洛阳玺宸商贸有限责任公司',\n",
       "  {'place': [['洛阳', 0, 2]],\n",
       "   'brand': [['玺宸', 2, 4]],\n",
       "   'trade': [['商贸', 4, 6]],\n",
       "   'suffix': [['有限责任公司', 6, 12]]}],\n",
       " ['广东银葵医院管理有限公司',\n",
       "  {'place': [['广东', 0, 2]],\n",
       "   'brand': [['银葵', 2, 4]],\n",
       "   'trade': [['医院管理', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['南靖明君企业管理合伙企业（有限合伙）',\n",
       "  {'place': [['南靖', 0, 2]],\n",
       "   'brand': [['明君', 2, 4]],\n",
       "   'trade': [['企业管理', 4, 8]],\n",
       "   'suffix': [['合伙企业', 8, 12], ['有限合伙', 13, 17]]}],\n",
       " ['广汉市伟业装饰工程有限公司',\n",
       "  {'place': [['广汉市', 0, 3]],\n",
       "   'brand': [['伟业', 3, 5]],\n",
       "   'trade': [['装饰工程', 5, 9]],\n",
       "   'suffix': [['有限公司', 9, 13]]}],\n",
       " ['锦州华源建筑有限公司',\n",
       "  {'place': [['锦州', 0, 2]],\n",
       "   'brand': [['华源', 2, 4]],\n",
       "   'trade': [['建筑', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['泉州华测检测服务有限公司',\n",
       "  {'place': [['泉州', 0, 2]],\n",
       "   'brand': [['华测', 2, 4]],\n",
       "   'trade': [['检测服务', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['上海花蝶日本料理有限公司',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['花蝶', 2, 4]],\n",
       "   'trade': [['日本料理', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['临沂程鸿机械配件有限公司',\n",
       "  {'place': [['临沂', 0, 2]],\n",
       "   'brand': [['程鸿', 2, 4]],\n",
       "   'trade': [['机械配件', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['苏州辉慈健康管理公司',\n",
       "  {'place': [['苏州', 0, 2]],\n",
       "   'brand': [['辉慈', 2, 4]],\n",
       "   'trade': [['健康管理', 4, 8]],\n",
       "   'suffix': [['公司', 8, 10]]}],\n",
       " ['江西鸿光凤凰实业有限公司',\n",
       "  {'place': [['江西', 0, 2]],\n",
       "   'brand': [['鸿光', 2, 4]],\n",
       "   'trade': [['凤凰实业', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['海南富乐佳投资管理有限公司',\n",
       "  {'place': [['海南', 0, 2]],\n",
       "   'brand': [['富乐佳', 2, 5]],\n",
       "   'trade': [['投资管理', 5, 9]],\n",
       "   'suffix': [['有限公司', 9, 13]]}],\n",
       " ['南京银康企业管理中心（有限合伙）',\n",
       "  {'place': [['南京', 0, 2]],\n",
       "   'brand': [['银康', 2, 4]],\n",
       "   'trade': [['企业管理', 4, 8]],\n",
       "   'suffix': [['中心', 8, 10], ['有限合伙', 11, 15]]}],\n",
       " ['武汉聚盛置业有限公司',\n",
       "  {'place': [['武汉', 0, 2]],\n",
       "   'brand': [['聚盛', 2, 4]],\n",
       "   'trade': [['置业', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['常州市易佳服装有限公司',\n",
       "  {'place': [['常州市', 0, 3]],\n",
       "   'brand': [['易佳', 3, 5]],\n",
       "   'trade': [['服装', 5, 7]],\n",
       "   'suffix': [['有限公司', 7, 11]]}],\n",
       " ['思八达企业发展(上海)有限公司',\n",
       "  {'brand': [['思八达', 0, 3]],\n",
       "   'trade': [['企业发展', 3, 7]],\n",
       "   'place': [['上海', 8, 10]],\n",
       "   'suffix': [['有限公司', 11, 15]]}],\n",
       " ['贺州常兴汽车销售有限公司',\n",
       "  {'place': [['贺州', 0, 2]],\n",
       "   'brand': [['常兴', 2, 4]],\n",
       "   'trade': [['汽车销售', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['成都平安虎家具有限公司',\n",
       "  {'place': [['成都', 0, 2]],\n",
       "   'brand': [['平安虎', 2, 5]],\n",
       "   'trade': [['家具', 5, 7]],\n",
       "   'suffix': [['有限公司', 7, 11]]}],\n",
       " ['冠军装饰有限责任公司',\n",
       "  {'brand': [['冠军', 0, 2]],\n",
       "   'trade': [['装饰', 2, 4]],\n",
       "   'suffix': [['有限责任公司', 4, 10]]}],\n",
       " ['陕西德源府谷能源有限公司',\n",
       "  {'place': [['陕西', 0, 2]],\n",
       "   'brand': [['德源', 2, 4]],\n",
       "   'trade': [['府谷', 4, 6], ['能源', 6, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['上海春塑实业有限公司',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['春塑', 2, 4]],\n",
       "   'trade': [['实业', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['江苏众赢商贸有限公司',\n",
       "  {'place': [['江苏', 0, 2]],\n",
       "   'brand': [['众赢', 2, 4]],\n",
       "   'trade': [['商贸', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['张家港玖沙钢铁贸易有限公司',\n",
       "  {'place': [['张家港', 0, 3]],\n",
       "   'brand': [['玖沙', 3, 5]],\n",
       "   'trade': [['钢铁贸易', 5, 9]],\n",
       "   'suffix': [['有限公司', 9, 13]]}],\n",
       " ['上海农工商田林超市有限公司',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'trade': [['农工商', 2, 5], ['超市', 7, 9]],\n",
       "   'brand': [['田林', 5, 7]],\n",
       "   'suffix': [['有限公司', 9, 13]]}],\n",
       " ['中航兰田汽车销售服务（上海）有限公司',\n",
       "  {'brand': [['中航兰田', 0, 4]],\n",
       "   'trade': [['汽车销售服务', 4, 10]],\n",
       "   'place': [['上海', 11, 13]],\n",
       "   'suffix': [['有限公司', 14, 18]]}],\n",
       " ['上海硕人广告企划有限公司',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['硕人', 2, 4]],\n",
       "   'trade': [['广告企划', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['淮安旭升智能制造有限公司',\n",
       "  {'place': [['淮安', 0, 2]],\n",
       "   'brand': [['旭升', 2, 4]],\n",
       "   'trade': [['智能制造', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['深圳亮启灯饰有限公司',\n",
       "  {'place': [['深圳', 0, 2]],\n",
       "   'brand': [['亮启', 2, 4]],\n",
       "   'trade': [['灯饰', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['北京创世蓝点国际文化传播有限公司',\n",
       "  {'place': [['北京', 0, 2]],\n",
       "   'brand': [['创世蓝点', 2, 6]],\n",
       "   'trade': [['国际文化传播', 6, 12]],\n",
       "   'suffix': [['有限公司', 12, 16]]}],\n",
       " ['陕西瀛泰文化传媒有限公司',\n",
       "  {'place': [['陕西', 0, 2]],\n",
       "   'brand': [['瀛泰', 2, 4]],\n",
       "   'trade': [['文化传媒', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['同益实业集团有限公司',\n",
       "  {'brand': [['同益', 0, 2]],\n",
       "   'trade': [['实业', 2, 4]],\n",
       "   'suffix': [['集团有限公司', 4, 10]]}],\n",
       " ['昆明奇峰机电设备有限公司',\n",
       "  {'place': [['昆明', 0, 2]],\n",
       "   'brand': [['奇峰', 2, 4]],\n",
       "   'trade': [['机电设备', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['杭州通信基站建设公司',\n",
       "  {'place': [['杭州', 0, 2]],\n",
       "   'trade': [['通信基站', 2, 6], ['建设', 6, 8]],\n",
       "   'suffix': [['公司', 8, 10]]}],\n",
       " ['邢台益淼商贸有限责任公司',\n",
       "  {'place': [['邢台', 0, 2]],\n",
       "   'brand': [['益淼', 2, 4]],\n",
       "   'trade': [['商贸', 4, 6]],\n",
       "   'suffix': [['有限责任公司', 6, 12]]}],\n",
       " ['上海青兰生物科技有限公司',\n",
       "  {'place': [['上海', 0, 2]],\n",
       "   'brand': [['青兰', 2, 4]],\n",
       "   'trade': [['生物科技', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['云南通广建设工程有限公司',\n",
       "  {'place': [['云南', 0, 2]],\n",
       "   'brand': [['通广', 2, 4]],\n",
       "   'trade': [['建设工程', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['北京泛亚电通工贸有限公司',\n",
       "  {'place': [['北京', 0, 2]],\n",
       "   'brand': [['泛亚', 2, 4]],\n",
       "   'trade': [['电通工贸', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['洛阳酒缘酒业商贸有限公司',\n",
       "  {'place': [['洛阳', 0, 2]],\n",
       "   'brand': [['酒缘', 2, 4]],\n",
       "   'trade': [['酒业商贸', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['南京味源食品有限公司',\n",
       "  {'place': [['南京', 0, 2]],\n",
       "   'brand': [['味源', 2, 4]],\n",
       "   'trade': [['食品', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['福州票付通信息科技有限公司',\n",
       "  {'place': [['福州', 0, 2]],\n",
       "   'brand': [['票付通', 2, 5]],\n",
       "   'trade': [['信息科技', 5, 9]],\n",
       "   'suffix': [['有限公司', 9, 13]]}],\n",
       " ['昆明广基物业管理有限公司',\n",
       "  {'place': [['昆明', 0, 2]],\n",
       "   'brand': [['广基', 2, 4]],\n",
       "   'trade': [['物业管理', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['重庆冠连体育设施有限公司',\n",
       "  {'place': [['重庆', 0, 2]],\n",
       "   'brand': [['冠连', 2, 4]],\n",
       "   'trade': [['体育设施', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['武汉长江资产经营公司',\n",
       "  {'place': [['武汉', 0, 2]],\n",
       "   'brand': [['长江', 2, 4]],\n",
       "   'trade': [['资产经营', 4, 8]],\n",
       "   'suffix': [['公司', 8, 10]]}],\n",
       " ['西南设计院南宁分公司',\n",
       "  {'brand': [['西南', 0, 2]],\n",
       "   'trade': [['设计院', 2, 5]],\n",
       "   'place': [['南宁', 5, 7]],\n",
       "   'suffix': [['分公司', 7, 10]]}],\n",
       " ['南京海锚电器制造公司',\n",
       "  {'place': [['南京', 0, 2]],\n",
       "   'brand': [['海锚', 2, 4]],\n",
       "   'trade': [['电器制造', 4, 8]],\n",
       "   'suffix': [['公司', 8, 10]]}],\n",
       " ['凯塞曼流体技术设备公司',\n",
       "  {'brand': [['凯塞曼', 0, 3]],\n",
       "   'trade': [['流体技术设备', 3, 9]],\n",
       "   'suffix': [['公司', 9, 11]]}],\n",
       " ['四川永泰体育设施有限公司',\n",
       "  {'place': [['四川', 0, 2]],\n",
       "   'brand': [['永泰', 2, 4]],\n",
       "   'trade': [['体育设施', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['陕西振豪消防工程有限公司',\n",
       "  {'place': [['陕西', 0, 2]],\n",
       "   'brand': [['振豪', 2, 4]],\n",
       "   'trade': [['消防工程', 4, 8]],\n",
       "   'suffix': [['有限公司', 8, 12]]}],\n",
       " ['青岛昌骏实业有限公司',\n",
       "  {'place': [['青岛', 0, 2]],\n",
       "   'brand': [['昌骏', 2, 4]],\n",
       "   'trade': [['实业', 4, 6]],\n",
       "   'suffix': [['有限公司', 6, 10]]}],\n",
       " ['深圳市中建南方建筑工程劳务有限公司',\n",
       "  {'place': [['深圳市', 0, 3]],\n",
       "   'brand': [['中建', 3, 5]],\n",
       "   'trade': [['南方', 5, 7], ['建筑工程', 7, 11], ['劳务', 11, 13]],\n",
       "   'suffix': [['有限公司', 13, 17]]}],\n",
       " ['四川君逸数码科技股份有限公司',\n",
       "  {'place': [['四川', 0, 2]],\n",
       "   'brand': [['君逸', 2, 4]],\n",
       "   'trade': [['数码科技', 4, 8]],\n",
       "   'suffix': [['股份有限公司', 8, 14]]}]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import convert_to_dict_pre\n",
    "\n",
    "class CompanyPredict():\n",
    "    def __init__(self, model, config, tokenizer, word_pad_idx = 0) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        # BertTokenizer.from_pretrained(config.bert_model, do_lower_case=True)\n",
    "        self.label2id = config.label2id\n",
    "        self.id2label = {_id: _label for _label, _id in list(config.label2id.items())}\n",
    "        self.word_pad_idx = word_pad_idx\n",
    "        self.device = config.device\n",
    "\n",
    "    def data_process(self, company_names):\n",
    "        sentences_ori = []\n",
    "        sentences = []\n",
    "        for company in company_names:\n",
    "            words = ['[CLS]'] + tokenizer.tokenize(company)\n",
    "            sentences_ori.append(words)\n",
    "            word_lens = []\n",
    "            for w in words[1:]: #防止有英文\n",
    "                word_lens.append(1) #len(w))\n",
    "            token_start_idxs = 1 + np.cumsum([0] + word_lens[:-1])\n",
    "            sentences.append((tokenizer.convert_tokens_to_ids(words), token_start_idxs))\n",
    "        # print(list(zip(sentences_ori[1][1:], sentences[1][-1])))\n",
    "\n",
    "        # print(list(zip(words[1:], sentences[-1][-1])))\n",
    "        # batch length\n",
    "        batch_len = len(sentences)\n",
    "\n",
    "        # compute length of longest sentence in batch\n",
    "        max_len = max([len(s[0]) for s in sentences])\n",
    "\n",
    "        # padding data 初始化\n",
    "        batch_data = self.word_pad_idx * np.ones((batch_len, max_len))\n",
    "        max_label_len = 0\n",
    "        batch_label_starts = []\n",
    "        sent_data = []\n",
    "        # padding and aligning\n",
    "        for j in range(batch_len):\n",
    "            cur_len = len(sentences[j][0])\n",
    "            batch_data[j][:cur_len] = sentences[j][0] #sentence\n",
    "            # 找到有标签的数据的index（[CLS]不算）\n",
    "            label_start_idx = sentences[j][-1] # word start index\n",
    "            label_starts = np.zeros(max_len)\n",
    "            label_starts[[idx for idx in label_start_idx if idx < max_len]] = 1\n",
    "            batch_label_starts.append(label_starts)\n",
    "            max_label_len = max(int(sum(label_starts)), max_label_len)\n",
    "\n",
    "        # convert data to torch LongTensors\n",
    "        batch_data = torch.tensor(np.array(batch_data), dtype=torch.long)\n",
    "        batch_label_starts = torch.tensor(np.array(batch_label_starts), dtype=torch.long)\n",
    "        #         print(batch_data.shape, batch_labels.shape)\n",
    "        #         print(batch_data, batch_labels)\n",
    "        # shift tensors to GPU if available\n",
    "        batch_data, batch_label_starts = batch_data.to(self.device), batch_label_starts.to(self.device)\n",
    "        \n",
    "        return batch_data, batch_label_starts, sentences_ori\n",
    "    \n",
    "    def predict(self, batch_data, batch_label_starts, sentences):\n",
    "        batch_masks = batch_data.gt(0)\n",
    "\n",
    "        batch_output = self.model((batch_data, batch_label_starts),\n",
    "                token_type_ids=None, attention_mask=batch_masks)\n",
    "\n",
    "        embedding = batch_output[1]\n",
    "        batch_output = batch_output[0]\n",
    "\n",
    "        batch_output = self.model.crf.viterbi_decode(batch_output, mask=batch_masks[:, 1:])\n",
    "\n",
    "        pred_tags = [[self.id2label.get(idx) for idx in indices] for indices in batch_output]\n",
    "        \n",
    "        all_end = []\n",
    "        for pred_label, senten in zip(pred_tags, sentences):\n",
    "            res = convert_to_dict_pre(senten[1:], pred_label)\n",
    "            for label in ['place', 'brand', 'trade', 'suffix']:\n",
    "                if label not in res.keys():\n",
    "                    res[label] = []\n",
    "                \n",
    "            all_end.append(res)\n",
    "        # all_end = predict_output_pre(pred_tags, sentences_ori)\n",
    "        return all_end, pred_tags, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import (\n",
    "  BertTokenizerFast,\n",
    ")\n",
    "import config\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "from zhconv import convert \n",
    "tokenizer = BertTokenizerFast.from_pretrained('./bert-base-chinese')\n",
    "model = torch.load('experiments/s_model/best_model/model.bin')\n",
    "# model = torch.load('experiments/s_model/last_model/model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained('glm-large-chinese', trust_remote_code=True)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained('glm-large-chinese', trust_remote_code=True)\n",
    "# model = model.half().cuda()\n",
    "# model.eval()\n",
    "# # Inference\n",
    "# inputs = tokenizer(\"Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.\", return_tensors=\"pt\")\n",
    "# inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=512)\n",
    "# inputs = inputs.to('cuda')\n",
    "# outputs = model.generate(**inputs, max_length=512, eos_token_id=tokenizer.eop_token_id)\n",
    "# print(tokenizer.decode(outputs[0].tolist()))\n",
    "# # Training\n",
    "# inputs = tokenizer(    [\"Tsinghua University is located in [MASK].\", \"One minus one equals zero, is it correct? Answer: [MASK]\"],    return_tensors=\"pt\", padding=True)\n",
    "# inputs = tokenizer.build_inputs_for_generation(inputs, targets=[\"Beijing\", \"No\"], max_gen_length=8, padding=False)\n",
    "# inputs = inputs.to('cuda')\n",
    "# outputs = model(**inputs)\n",
    "# loss = outputs.losslogits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dirs = os.listdir('./data/')\n",
    "del_dir = []\n",
    "for file_dir in file_dirs:\n",
    "    if not file_dir.isnumeric():\n",
    "        del_dir.append(file_dir)\n",
    "file_dirs = list(set(file_dirs) - set(del_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dirs.sort(reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(string):\n",
    "    for ch in string:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "company_ids = []\n",
    "company_names = []\n",
    "with open('blacklist_data/20231105/HMD_SA29_RECORD_LIST_20231105.dat', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split('@!@')\n",
    "        l_id = l[0]\n",
    "        l_name = convert(l[1].strip().replace(\"“\",'\"').replace(\"”\",'\"'), 'zh-cn')\n",
    "        if not is_chinese(l_name):\n",
    "            continue\n",
    "        l_name = re.sub(r'[^\\u4e00-\\u9fffa-zA-Z0-9()（）\"\\'“”‘’]', ' ', l_name.lower())\n",
    "        company_ids.append(l_id)\n",
    "        company_names.append(l_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_names = []\n",
    "# with open('data/name-sample-check.json', 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         l = json.loads(line.strip())\n",
    "#         company_names.append(l['text'].strip().replace(\"“\",'\"').replace(\"”\",'\"'))\n",
    "# company_names = ['福建省福州高速公路管理公司', \n",
    "#                  '亚马逊信息服务(北京)有限公司',\n",
    "#                  '汕头嘉祥塑料制品厂有限公司', \n",
    "#                  'happy昱景科技股份有限公司', \n",
    "#                  '雷蒙药品经销有限责任公司',\n",
    "#                  '上海农商银行',\n",
    "#                  '中国农业银行',\n",
    "#                  '温州银行\"金鹿理财－普惠颐养\"２１０５０号开放式净值型银行理财计划',\n",
    "#                  '卓越成长定增基金1号',\n",
    "#                  '上海雄艮企业管理中心（有限合伙）']\n",
    "# company_names = ['上行', '工商银行', '农行', '上海银行', '上海农商银行', '中国农业银行', '浦发银行', '阿里巴巴', '阿里', '腾讯', '邦盛科技', '银行']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(company_names) / 128) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:04<00:00, 10.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_dir = config.model_dir + '/best'\n",
    "# model = torch.load('experiments/s_model/model.bin')\n",
    "# # BertNER.from_pretrained(model_dir)\n",
    "# model.to(config.device)\n",
    "CPredict = CompanyPredict(model, config, tokenizer)\n",
    "\n",
    "results = []\n",
    "part = 128\n",
    "iter_num = int(len(company_names) / part) + 1\n",
    "embeddings = []\n",
    "for i in tqdm(range(iter_num)):\n",
    "    start = i * part \n",
    "    if i == iter_num -1:\n",
    "        end = len(company_names)\n",
    "    else:\n",
    "        end = (i + 1) * part\n",
    "    batch_data, batch_label_starts, sentences_ori = \\\n",
    "        CPredict.data_process(company_names[start:end])\n",
    "    all_end, pred_tags, _ = CPredict.predict(batch_data, batch_label_starts, sentences_ori)\n",
    "    results.extend(all_end)\n",
    "    # embeddings.extend(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ori = []\n",
    "for i in range(len(results)):\n",
    "    results_ori.append({'id': company_ids[i], 'text': company_names[i], 'label': results[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'A143278',\n",
       " 'text': '上行普惠十三公司',\n",
       " 'label': {'brand': [['上行普惠', 0, 4], ['十三', 4, 6]],\n",
       "  'suffix': [['公司', 6, 8]],\n",
       "  'place': [],\n",
       "  'trade': []}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_levenshtein_distance(text1, text2):\n",
    "    m, n = len(text1), len(text2)\n",
    "    dp = np.zeros((m + 1, n + 1))\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if text1[i - 1] == text2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1\n",
    "\n",
    "    return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "string1 = \"汽车\"\n",
    "string2 = \"汽车制造\"\n",
    "sim_score = fuzz.ratio(string1, string2)\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 富源县营上镇大坪煤矿\n",
    "string1 = \"大众\"\n",
    "string2 = \"大坪\"\n",
    "sim_score = fuzz.ratio(string1, string2)\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPredict = CompanyPredict(model, config, tokenizer)\n",
    "\n",
    "target_text = '科技中心'\n",
    "# company_names = ['哈尔滨黄金自营有限公司', \n",
    "#                   '测试公司1', \n",
    "#                   '马宗仓自有资金有限公司', \n",
    "#                   '广西中金金属科技有限公司', \n",
    "#                   '柳州市元哥再生资源回收有限公司', \n",
    "#                   '温州银行\"金鹿理财－普惠颐养\"２１０５０号开放式净值型银行理财计划']\n",
    "batch_data, batch_label_starts, sentences_ori = \\\n",
    "        CPredict.data_process([target_text])\n",
    "all_end, pred_tags, embedding = CPredict.predict(batch_data, batch_label_starts, sentences_ori)\n",
    "\n",
    "target = {'text': target_text, 'label': all_end[0]}\n",
    "target_embedding = embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '科技中心',\n",
       " 'label': {'trade': [['科技', 0, 2]],\n",
       "  'suffix': [['中心', 2, 4]],\n",
       "  'place': [],\n",
       "  'brand': []}}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# target = {'text': '同里',\n",
    "#   'label': {'brand': [['同里', 0, 2]], 'place': [], 'trade': [], 'suffix': []}}\n",
    "candidates = []\n",
    "for result in results:\n",
    "    score = defaultdict(float)\n",
    "\n",
    "    # print(item['text'], target['text'],  len(set(item['text']) & set(target['text'])))\n",
    "    if len(set(result['text']) & set(target['text'])) == 0:\n",
    "        continue\n",
    "    if len(set(result['text']) & set(target['text'])) == len(target['text']):\n",
    "        score['score'] = 0\n",
    "        candidates.append([result['id'], result['text'], score['score']])\n",
    "        continue\n",
    "    \n",
    "    score['score'] = 0\n",
    "    for label in ['place', 'trade', 'brand', 'suffix']:\n",
    "        score[label] = 0\n",
    "        if target['label'][label] == [] and result['label'][label] == []:\n",
    "            continue\n",
    "\n",
    "        if target['label'][label] == [] and result['label'][label] != []:\n",
    "            # score[item['text']][label] += sum([len(candi[0]) for candi in item['label'][label]])\n",
    "            # score[item['text']]['score'] += score[item['text']][label]\n",
    "            continue\n",
    "\n",
    "        if target['label'][label] != [] and result['label'][label] == []:\n",
    "            score[label] += sum([len(candi[0]) for candi in target['label'][label]])\n",
    "        else:\n",
    "            target_text = ''.join([item[0] for item in target['label'][label]])\n",
    "            candi = ''.join([item[0] for item in result['label'][label]])\n",
    "            score[label] = calculate_levenshtein_distance(target_text, candi)\n",
    "        \n",
    "        if label == 'brand':\n",
    "            score[label] = score[label] * 2\n",
    "        if label == 'trade':\n",
    "            score[label] = score[label] * 1.5\n",
    "\n",
    "        score['score'] += score[label]\n",
    "\n",
    "    # if score[item['text']]['score'] < len(target['text']):\n",
    "    candidates.append([result['id'], result['text'], score['score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>A142615-1146</td>\n",
       "      <td>上海克皮网络科技中心</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>A142613-0880</td>\n",
       "      <td>广州子纯科技商行</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>A142615-0912</td>\n",
       "      <td>广州亦峰科技部</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>A142613-0284</td>\n",
       "      <td>广州地基科技商行</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>A142613-0204</td>\n",
       "      <td>广州裙利科技商行</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>A142613-0962</td>\n",
       "      <td>广州瑞正科技商行</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>A142613-0390</td>\n",
       "      <td>广州涛米科技店</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>dowjones836583</td>\n",
       "      <td>东土耳其斯坦信息中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>A142615-1152</td>\n",
       "      <td>白银区品格日化中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>A142615-1176</td>\n",
       "      <td>长沙酷峥物业中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>A142615-2144</td>\n",
       "      <td>杭州余杭区仓前街道妙玖科技工作室</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>A142615-2143</td>\n",
       "      <td>杭州余杭区仓前街道吟白科技工作室</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>A142615-2826</td>\n",
       "      <td>杭州余杭区仓前街道洛却科技工作室</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>A142615-2827</td>\n",
       "      <td>杭州余杭区仓前街道骏怡科技工作室</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>A142615-1587</td>\n",
       "      <td>抚州夕新贸易中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>A142613-0519</td>\n",
       "      <td>北海高畅保洁中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>A142613-0760</td>\n",
       "      <td>石家庄无隆商贸中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>dowjones836583</td>\n",
       "      <td>东土耳其斯坦信息中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>dowjones836583</td>\n",
       "      <td>东突信息中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>A142615-0997</td>\n",
       "      <td>无极县琛拓贸易中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>A142613-0954</td>\n",
       "      <td>桂林成邦科技工作室</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>dowjones836583</td>\n",
       "      <td>东突信息中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>A142615-2310</td>\n",
       "      <td>武汉潇桦岚贸易中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>A142615-2311</td>\n",
       "      <td>武汉仟落莱贸易中心</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>A142615-0708</td>\n",
       "      <td>武汉速越科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>A142615-1942</td>\n",
       "      <td>天津天之承科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>A142615-0633</td>\n",
       "      <td>杭州栗壬科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>A142615-1061</td>\n",
       "      <td>成都爱一路科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>A142613-0662</td>\n",
       "      <td>大庆华之雷科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>A142615-1412</td>\n",
       "      <td>厦门参湾源科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>A142615-2663</td>\n",
       "      <td>南京罗斯狄科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>A142615-2600</td>\n",
       "      <td>贵溪瑞辛科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>A142615-0229</td>\n",
       "      <td>杭州棕泰科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>A142615-2573</td>\n",
       "      <td>贵州锋双宏科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>A142613-0447</td>\n",
       "      <td>成都开互鑫科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>A142615-0690</td>\n",
       "      <td>厦门春步肖科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>A142615-2151</td>\n",
       "      <td>深圳市鸿柯通科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>A142615-1789</td>\n",
       "      <td>唐山轩北科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>A142615-1704</td>\n",
       "      <td>哈尔滨寒畅科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>A142615-1705</td>\n",
       "      <td>武汉天皓羽美科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>A142613-0408</td>\n",
       "      <td>广州明宇东科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>A142613-0577</td>\n",
       "      <td>成都凝斯额科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>A142613-0700</td>\n",
       "      <td>广州星化科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>A142613-0575</td>\n",
       "      <td>广州聪梅科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>A142615-2793</td>\n",
       "      <td>广西海峰科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>A142615-2667</td>\n",
       "      <td>智辉祥（天津）科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>A142613-0590</td>\n",
       "      <td>合肥林立科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>A142615-0457</td>\n",
       "      <td>贵州含泡孺科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>A142615-2572</td>\n",
       "      <td>贵州若程科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>A142615-1916</td>\n",
       "      <td>黑龙江晶琪科技有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                 1    2\n",
       "461     A142615-1146        上海克皮网络科技中心  0.0\n",
       "829     A142613-0880          广州子纯科技商行  2.0\n",
       "1094    A142615-0912           广州亦峰科技部  2.0\n",
       "921     A142613-0284          广州地基科技商行  2.0\n",
       "963     A142613-0204          广州裙利科技商行  2.0\n",
       "663     A142613-0962          广州瑞正科技商行  2.0\n",
       "951     A142613-0390           广州涛米科技店  2.0\n",
       "98    dowjones836583        东土耳其斯坦信息中心  3.0\n",
       "530     A142615-1152         白银区品格日化中心  3.0\n",
       "801     A142615-1176          长沙酷峥物业中心  3.0\n",
       "890     A142615-2144  杭州余杭区仓前街道妙玖科技工作室  3.0\n",
       "839     A142615-2143  杭州余杭区仓前街道吟白科技工作室  3.0\n",
       "796     A142615-2826  杭州余杭区仓前街道洛却科技工作室  3.0\n",
       "797     A142615-2827  杭州余杭区仓前街道骏怡科技工作室  3.0\n",
       "661     A142615-1587          抚州夕新贸易中心  3.0\n",
       "810     A142613-0519          北海高畅保洁中心  3.0\n",
       "911     A142613-0760         石家庄无隆商贸中心  3.0\n",
       "100   dowjones836583        东土耳其斯坦信息中心  3.0\n",
       "101   dowjones836583            东突信息中心  3.0\n",
       "431     A142615-0997         无极县琛拓贸易中心  3.0\n",
       "760     A142613-0954         桂林成邦科技工作室  3.0\n",
       "99    dowjones836583            东突信息中心  3.0\n",
       "267     A142615-2310         武汉潇桦岚贸易中心  3.0\n",
       "268     A142615-2311         武汉仟落莱贸易中心  3.0\n",
       "954     A142615-0708        武汉速越科技有限公司  4.0\n",
       "242     A142615-1942       天津天之承科技有限公司  4.0\n",
       "451     A142615-0633        杭州栗壬科技有限公司  4.0\n",
       "453     A142615-1061       成都爱一路科技有限公司  4.0\n",
       "955     A142613-0662       大庆华之雷科技有限公司  4.0\n",
       "952     A142615-1412       厦门参湾源科技有限公司  4.0\n",
       "247     A142615-2663       南京罗斯狄科技有限公司  4.0\n",
       "244     A142615-2600        贵溪瑞辛科技有限公司  4.0\n",
       "450     A142615-0229        杭州棕泰科技有限公司  4.0\n",
       "255     A142615-2573       贵州锋双宏科技有限公司  4.0\n",
       "804     A142613-0447       成都开互鑫科技有限公司  4.0\n",
       "812     A142615-0690       厦门春步肖科技有限公司  4.0\n",
       "936     A142615-2151      深圳市鸿柯通科技有限公司  4.0\n",
       "811     A142615-1789        唐山轩北科技有限公司  4.0\n",
       "938     A142615-1704       哈尔滨寒畅科技有限公司  4.0\n",
       "939     A142615-1705      武汉天皓羽美科技有限公司  4.0\n",
       "940     A142613-0408       广州明宇东科技有限公司  4.0\n",
       "263     A142613-0577       成都凝斯额科技有限公司  4.0\n",
       "250     A142613-0700        广州星化科技有限公司  4.0\n",
       "262     A142613-0575        广州聪梅科技有限公司  4.0\n",
       "444     A142615-2793        广西海峰科技有限公司  4.0\n",
       "258     A142615-2667     智辉祥（天津）科技有限公司  4.0\n",
       "944     A142613-0590        合肥林立科技有限公司  4.0\n",
       "808     A142615-0457       贵州含泡孺科技有限公司  4.0\n",
       "254     A142615-2572        贵州若程科技有限公司  4.0\n",
       "253     A142615-1916       黑龙江晶琪科技有限公司  4.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(candidates).sort_values(by=2)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def cal_cosine_similarity(vector1, vector2):\n",
    "#     cos_sim = vector1.dot(vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "#     return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '邯郸腾信广告有限公司'\n",
    "batch_data, batch_label_starts, sentences_ori = \\\n",
    "        CPredict.data_process([text])\n",
    "all_end, pred_tags, embedding = CPredict.predict(batch_data, batch_label_starts, sentences_ori)\n",
    "\n",
    "target = all_end[0]\n",
    "target_embedding = embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0977, -0.1592,  1.0096,  ..., -0.0745, -0.4246, -1.4060],\n",
       "        [-0.3302, -0.4835,  0.1196,  ...,  1.0540,  0.6491, -0.2148]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embedding[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "score = defaultdict(dict)\n",
    "candidates = []\n",
    "for i in range(len(results)):\n",
    "    item = results[i]\n",
    "    if len(set(item['text']) & set(target['text'])) == 0:\n",
    "        continue\n",
    "    score[item['text']]['score'] = 0\n",
    "    for label in ['place', 'trade', 'brand', 'suffix']:\n",
    "        score[item['text']][label] = 0\n",
    "        if target['label'][label] == [] and item['label'][label] == []:\n",
    "            continue\n",
    "\n",
    "        if target['label'][label] == [] and item['label'][label] != []:\n",
    "            # score[item['text']][label] += sum([len(candi[0]) for candi in item['label'][label]])\n",
    "            # score[item['text']]['score'] += score[item['text']][label]\n",
    "            continue\n",
    "\n",
    "        if target['label'][label] != [] and item['label'][label] == []:\n",
    "            # score[item['text']][label] += 0\n",
    "            # score[item['text']]['score'] += score[item['text']][label]\n",
    "            continue\n",
    "\n",
    "        for target_text in target['label'][label]:\n",
    "            for candi in item['label'][label]:\n",
    "                score[item['text']][label] += cal_cosine_similarity(target_embedding[target_text[1]: target_text[2]].sum(0).cpu().detach().numpy(), \\\n",
    "                                                                embeddings[i][candi[1]: candi[2]].sum(0).cpu().detach().numpy())\n",
    "        score[item['text']]['score'] += score[item['text']][label]\n",
    "\n",
    "        candidates.append([item['text'], score[item['text']]['score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>邯郸腾信广告有限公司</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>陕西嘉禾广告有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>合肥十方广告有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>四川笑苑广告有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>上海森林广告有限公司</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>吉林市华阳机电设备有限公司</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>吉林省衡运文化传媒有限公司</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>台州市海森泰进出口有限公司</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>常州市精创塑料科技有限公司</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>成都怡讯通电子设备有限公司</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>845 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0    1\n",
       "274     邯郸腾信广告有限公司  0.0\n",
       "698     陕西嘉禾广告有限公司  4.0\n",
       "429     合肥十方广告有限公司  4.0\n",
       "377     四川笑苑广告有限公司  4.0\n",
       "824     上海森林广告有限公司  4.0\n",
       "..             ...  ...\n",
       "414  吉林市华阳机电设备有限公司  9.0\n",
       "630  吉林省衡运文化传媒有限公司  9.0\n",
       "633  台州市海森泰进出口有限公司  9.0\n",
       "339  常州市精创塑料科技有限公司  9.0\n",
       "337  成都怡讯通电子设备有限公司  9.0\n",
       "\n",
       "[845 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rr = pd.DataFrame(candidates)\n",
    "rr.sort_values(by=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0520, -0.0061, -0.0453,  ..., -0.0565,  0.0668,  0.0365],\n",
       "        [-0.0328,  0.1007, -0.1391,  ..., -0.0254,  0.0855,  0.0592],\n",
       "        [-0.0946,  0.1788, -0.1453,  ...,  0.0084,  0.0660,  0.0519],\n",
       "        [-0.1386,  0.2256, -0.1158,  ...,  0.0175,  0.0366,  0.0286]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embedding[target_text[1]: target_text[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = embedding[-5].sum(0)\n",
    "b = embedding[-4].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim1 = cosine_similarity(a.cpu().detach().numpy().reshape(1, -1), b.cpu().detach().numpy().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8863816]], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = embedding[0][:2].sum(0)\n",
    "a2 = embedding[3].sum(0)\n",
    "a3 = embedding[4].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/shbank_company_split.txt', 'w', encoding='utf-8') as f:\n",
    "#     for res, com in zip(results, company_names):\n",
    "#         res['text'] = com\n",
    "#         f.write(json.dumps(res, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim1 = cosine_similarity(a1.cpu().detach().numpy().reshape(1, -1), a2.cpu().detach().numpy().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim2 = cosine_similarity(a1.cpu().detach().numpy().reshape(1, -1), a3.cpu().detach().numpy().reshape(1, -1))\n",
    "cos_sim3 = cosine_similarity(a2.cpu().detach().numpy().reshape(1, -1), a3.cpu().detach().numpy().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'annoy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mannoy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnnoyIndex\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m768\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'annoy'"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import random\n",
    "f = 768\n",
    "t = AnnoyIndex(f, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28189948]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9458423]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "ss = re.sub(r'[^\\u4e00-\\u9fffa-zA-Z0-9()（）\"\\'“”‘’]', '', 'Hello世界(123)--（456）“”‘\"\\'#$%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello世界(123)（456）“”‘\"\\''"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ea19d11efa7602c1f12500925a974ed4f31fcf847bd6f694bd5180da2602ded"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
